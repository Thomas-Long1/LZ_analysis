{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ada88e2-fcbd-430c-be34-37379049e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################IMPORTS##################################\n",
    "import glob\n",
    "import uproot as up\n",
    "import awkward as ak\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tqdm as tqdm\n",
    "import multiprocessing as mp\n",
    "import functools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from operator import add\n",
    "from matplotlib import rcParams\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd7a8d3-8eec-42f2-8580-edefd97cd190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-19 11:21:18.436421: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-19 11:21:18.462090: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /cvmfs/grid.cern.ch/centos7-umd4-wn-4.0.5-1_191112/lib64:/cvmfs/grid.cern.ch/centos7-umd4-wn-4.0.5-1_191112/lib:/cvmfs/grid.cern.ch/centos7-umd4-wn-4.0.5-1_191112/usr/lib64:/cvmfs/grid.cern.ch/centos7-umd4-wn-4.0.5-1_191112/usr/lib\n",
      "2022-04-19 11:21:18.462116: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "#######################################CONFIGURATION##################################\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=32, \n",
    "                        inter_op_parallelism_threads=32, \n",
    "                        allow_soft_placement=True)\n",
    "\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c587b3-20de-4d2a-b8e4-484e987fa295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################DataAquisitionAndWrangling#####################\n",
    "\n",
    "def data_decay(file):\n",
    "    \"\"\"\n",
    "    Function to open up and flatten root files\n",
    "    \"\"\"\n",
    "    \n",
    "    tfile= up.open(file)\n",
    "    events=tfile[\"Events\"]\n",
    "    \n",
    "    area=np.array(events[\"pulsesODHG.pulseArea_phd\"])\n",
    "    area=area.flatten()\n",
    "    \n",
    "    truth = tfile['RQMCTruth']\n",
    "    pp = np.array(truth['mcTruthEvent./mcTruthEvent.parentParticle'])\n",
    "    pp=pp.flatten()\n",
    "    \n",
    "    coincidence=np.array(events[\"pulsesODHG.coincidence\"])\n",
    "    coincidence=coincidence.flatten()\n",
    "    \n",
    "    \n",
    "    peak_time=np.array(events[\"pulsesODHG.peakTime_ns\"])\n",
    "    peak_time=peak_time.flatten()\n",
    "       \n",
    "\n",
    "    peak_amp=np.array(events[\"pulsesODHG.peakAmp\"])\n",
    "    peak_amp=peak_amp.flatten()\n",
    "    \n",
    "    \n",
    "    t_25=np.array(events[\"pulsesODHG.areaFractionTime25_ns\"])\n",
    "    t_25=t_25.flatten()\n",
    "    \n",
    "    \n",
    "    t_50=np.array(events[\"pulsesODHG.areaFractionTime50_ns\"])\n",
    "    t_50=t_50.flatten()\n",
    "    \n",
    "    \n",
    "    t_75=np.array(events[\"pulsesODHG.areaFractionTime75_ns\"])\n",
    "    t_75=t_75.flatten()\n",
    "    \n",
    "    \n",
    "    start=np.array(events[\"pulsesODHG.pulseStartTime_ns\"])\n",
    "    start=start.flatten()\n",
    "    \n",
    "    \n",
    "    end=np.array(events[\"pulsesODHG.pulseEndTime_ns\"])\n",
    "    end=end.flatten()\n",
    "    \n",
    "    \n",
    "    length=end-start\n",
    "    area_time=t_75/area\n",
    "    \n",
    "    return [area,coincidence,peak_time,peak_amp,t_25,t_50,t_75,length,area_time,pp]\n",
    "\n",
    "def data():\n",
    "    \"\"\"\n",
    "    Loops over all root files and turns data into pandas arrays\n",
    "    \"\"\"\n",
    "    columns={'Pulse Area' : [], 'Coincidence' : [], 'Peak Time' : [],'Peak Amp' : [], \n",
    "             '25% time' : [], '50% time' : [],'75% time' : [], 'Pulse Time' : [], 'time/area' : [],'tag' : []}\n",
    "    columns_exp=['Pulse Area','Coincidence','Peak Time','Peak Amp','25% time',\n",
    "             '50% time', '75% time', 'Pulse Time', 'time/area']\n",
    "    files = glob.glob('/hdfs/user/ak18773/od_simulations/BACCARAT_6.2.14_DER_9.1.0_LZAP_5.4.1/od_internals/lzap_output/*')\n",
    "    with mp.Pool(30) as pool: # uses pool multiprocessing for parallellisation\n",
    "            internals_data = list(tqdm.tqdm(pool.imap(functools.partial(data_decay),files),total=3000))#uses tqdm to time aquisition\n",
    "    internals_data=pd.DataFrame(np.concatenate(internals_data,axis=1).T.tolist(),columns=columns)\n",
    "    internals_data=(((internals_data.explode(columns_exp)).reset_index().dropna())).drop(columns='index')\n",
    "    internals_data=internals_data[(internals_data[\"Pulse Area\"] > 5)&(internals_data['Coincidence'] >1)]\n",
    "    \n",
    "    files = glob.glob('/hdfs/user/ak18773/od_simulations/BACCARAT_6.2.14_DER_9.1.0_LZAP_5.4.1/gdls_neutrons/lzap_output/*')\n",
    "    with mp.Pool(30) as pool:\n",
    "            neutron_data = list(tqdm.tqdm(pool.imap(functools.partial(data_decay),files),total=1962))\n",
    "    neutron_data=pd.DataFrame(np.concatenate(neutron_data,axis=1).T.tolist(),columns=columns)\n",
    "    neutron_data=(((neutron_data.explode(columns_exp)).reset_index().dropna())).drop(columns='index')\n",
    "    neutron_data=neutron_data[(neutron_data[\"Pulse Area\"] > 5)&(neutron_data['Coincidence'] >1)]\n",
    "    \n",
    "    files = glob.glob('/hdfs/user/ak18773/od_simulations/BACCARAT_6.2.14_DER_9.1.0_LZAP_5.4.1/scinttank_rockgamma_all/lzap_output/*')\n",
    "    with mp.Pool(30) as pool:\n",
    "            rockgamma_data = list(tqdm.tqdm(pool.imap(functools.partial(data_decay),files),total=116))\n",
    "    rockgamma_data=pd.DataFrame(np.concatenate(rockgamma_data,axis=1).T.tolist(),columns=columns)\n",
    "    rockgamma_data=(((rockgamma_data.explode(columns_exp)).reset_index().dropna())).drop(columns='index')\n",
    "    rockgamma_data=rockgamma_data[(rockgamma_data[\"Pulse Area\"] > 5)&(rockgamma_data['Coincidence'] >1)]\n",
    "    \n",
    "    return internals_data,neutron_data,rockgamma_data\n",
    "\n",
    "def separate(internals_data,neutron_data,rockgamma_data):\n",
    "    \"\"\"\n",
    "    Separates dataframes into decay type and chain\n",
    "    \"\"\"\n",
    "    \n",
    "    Th232_chain_tag=['Th232','Ra228','Ac228','Th228','Ra224','Rn220',\n",
    "             'Po216','Po212','Bi212','Pb212','Pb208','Ti208']\n",
    "\n",
    "    U238_chain_tag=['U238','Th234','Pa234','U234','Th230','Ra226',\n",
    "                'Rn222','Po218','Pb214','At218','Bi214','Ti210','Po214',\n",
    "                'Pb210','Hg206','Bi210','Ti206','Po210','Pb206']\n",
    "\n",
    "    electrons_tag=['Th234','Pa234','Bi214','Ti210','Pb210','Bi210',\n",
    "           'Ti206','Ra228','Ac228','Pb212','Bi212','Ti208']\n",
    "\n",
    "    alphas_tag=['U238','U234','Th230','Ra226','Rn222','Po218','At218',\n",
    "        'Bi214','Po214','Pb210','Bi210','Po210','Th232','Th228',\n",
    "        'Ra224','Rn220','Po216','Bi212','Po212']\n",
    "\n",
    "\n",
    "    Th232_chain=internals_data[internals_data['tag'].isin(Th232_chain_tag)]\n",
    "    U238_chain=internals_data[internals_data['tag'].isin(U238_chain_tag)]\n",
    "    U238_Th232_chain=pd.concat([Th232_chain,U238_chain])\n",
    "    reminder=(pd.DataFrame(internals_data).merge(pd.DataFrame(U238_Th232_chain), how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']).drop(columns=['_merge']) \n",
    "    reminder=reminder[reminder['tag'] != \"Gd152\"]\n",
    "    U238_chain=U238_chain.drop(columns=[\"tag\"])\n",
    "    U238_chain['tag']='U238'\n",
    "    \n",
    "    Th232_chain=Th232_chain.drop(columns=[\"tag\"])\n",
    "    Th232_chain['tag']='Th232'\n",
    "\n",
    "    electrons=internals_data[internals_data['tag'].isin(electrons_tag)]\n",
    "    alphas=internals_data[internals_data['tag'].isin(alphas_tag)]\n",
    "    electons_alphas=pd.concat([electrons,alphas])\n",
    "    reminder_type=(pd.DataFrame(internals_data).merge(pd.DataFrame(electons_alphas), how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']).drop(columns=['_merge']) \n",
    "    reminder_type=reminder_type[reminder_type['tag'] != \"Gd152\"]\n",
    "    electrons=electrons.drop(columns=[\"tag\"])\n",
    "    alphas=alphas.drop(columns=[\"tag\"])\n",
    "    electrons['tag']='electron'\n",
    "    alphas['tag']='alpha'\n",
    "    return Th232_chain,U238_chain,reminder, electrons, alphas,reminder_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c42d1f-a122-412e-8b55-495ddefa9c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################DataVisualisation#####################################\n",
    "def plot(Data1,Data2,Data3,Data4,Data,label1,label2,label3,label4):\n",
    "    \"\"\"\n",
    "    Plots various aspects of dataframes\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "    axes[0,0].scatter(Data1['Pulse Area'],Data1['Coincidence'],label=label1,alpha=0.2)\n",
    "\n",
    "    axes[0,0].set_xlabel(\"Area\")\n",
    "    axes[0,0].set_ylabel(\"coincidence\")\n",
    "    axes[0,0].legend(loc='lower right')\n",
    "    axes[0,0].set_title(\"pulse area against coincidence\")\n",
    "\n",
    "\n",
    "\n",
    "    axes[0,1].scatter(Data2['Pulse Area'],Data2['Coincidence'],label=label2,alpha=0.2,color=\"orange\")\n",
    "\n",
    "    axes[0,1].set_xlabel(\"Area\")\n",
    "    axes[0,1].set_ylabel(\"coincidence\")\n",
    "    axes[0,1].legend(loc='lower right')\n",
    "    axes[0,1].set_title(\"pulse area against coincidence\")\n",
    "\n",
    "\n",
    "    axes[1,0].scatter(Data3['Pulse Area'],Data3['Coincidence'],label=label3,alpha=0.2,color=\"r\")\n",
    "    axes[1,0].set_xlabel(\"Area\")\n",
    "    axes[1,0].set_ylabel(\"coincidence\")\n",
    "    axes[1,0].legend(loc='lower right')\n",
    "    axes[1,0].set_title(\"pulse area against coincidence\")\n",
    "\n",
    "    axes[1,1].scatter(Data4['Pulse Area'],Data4['Coincidence'],label=label4,alpha=0.2)\n",
    "    axes[1,1].set_xlabel(\"Pulse Area\")\n",
    "    axes[1,1].set_ylabel(\"Coincidence\")\n",
    "    axes[1,1].legend(loc='lower right')\n",
    "    axes[1,1].set_title(\"area against coincidence\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "    axes[0,0].scatter(Data1['Pulse Area'],Data1['Peak Amp'],label=label1,alpha=0.3)\n",
    "    axes[0,0].scatter(Data2['Pulse Area'],Data2['Peak Amp'],label=label2,alpha=0.3)\n",
    "    axes[0,0].scatter(Data3['Pulse Area'],Data3['Peak Amp'],label=label3,alpha=0.3)\n",
    "    axes[0,0].scatter(Data4['Pulse Area'],Data4['Peak Amp'],label=label4,alpha=0.3)\n",
    "    axes[0,0].set_xlabel(\"Pulse area\")\n",
    "    axes[0,0].set_ylabel(\"peak current\")\n",
    "    axes[0,0].legend(loc='upper right')\n",
    "    axes[0,0].set_title(\"pulse area vs peak current\")\n",
    "\n",
    "\n",
    "    axes[0,1].scatter(Data1['Coincidence'],Data1['Peak Amp'],label=label1,alpha=0.2)\n",
    "    axes[0,1].scatter(Data2['Coincidence'],Data2['Peak Amp'],label=label2,alpha=0.2)\n",
    "    axes[0,1].scatter(Data3['Coincidence'],Data3['Peak Amp'],label=label3,alpha=0.2)\n",
    "    axes[0,1].scatter(Data4['Coincidence'],Data4['Peak Amp'],label=label4,alpha=0.2)\n",
    "    axes[0,1].set_xlabel(\"Coincidence\")\n",
    "    axes[0,1].set_ylabel(\"peak current\")\n",
    "    axes[0,1].legend(loc='upper right')\n",
    "    axes[0,1].set_title(\"coincidence vs peak current\")\n",
    "\n",
    "    axes[1,0].hist(Data1['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500], label=label1)\n",
    "    axes[1,0].hist(Data2['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500],label=label2)\n",
    "    axes[1,0].hist(Data3['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500],label=label3)\n",
    "    axes[1,0].hist(Data4['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500],label=label4)\n",
    "    axes[1,0].set_xlabel(\"Pulse Area\")\n",
    "    axes[1,0].legend(loc='upper right')\n",
    "    axes[1,0].set_title(\"Pulse Area\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    axes[1,1].hist(Data1['Pulse Time'], alpha = 0.7,bins=55,label=label1)\n",
    "    axes[1,1].hist(Data2['Pulse Time'], alpha = 0.7,bins=55,label=label2)\n",
    "    axes[1,1].hist(Data3['Pulse Time'], alpha = 0.7,bins=55,label=label3)\n",
    "    axes[1,1].hist(Data4['Pulse Time'], alpha = 0.7,bins=55,label=label4)\n",
    "    axes[1,1].set_xlabel(\"Pulse Time\")\n",
    "    axes[1,1].legend(loc='upper right')\n",
    "    axes[1,1].set_title(\"Pulse Time\")\n",
    "    plt.show()\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes[0,0].hist(Data1['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500], label=label1)\n",
    "    axes[0,0].hist(Data2['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500],label=label2)\n",
    "    axes[0,0].hist(Data3['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500],label=label3)\n",
    "    axes[0,0].hist(Data4['Pulse Area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,500],label=label4)\n",
    "\n",
    "    axes[0,1].hist(Data1['Coincidence'],histtype='step',bins=100,stacked=True ,density=True, label=label1)\n",
    "    axes[0,1].hist(Data2['Coincidence'],histtype='step',bins=100,stacked=True ,density=True,label=label2)\n",
    "    axes[0,1].hist(Data3['Coincidence'],histtype='step',bins=100,stacked=True ,density=True,label=label3)\n",
    "    axes[0,1].hist(Data4['Coincidence'],histtype='step',bins=100,stacked=True ,density=True,label=label4)\n",
    "                         \n",
    "    axes[0,2].hist(Data1['Peak Time'],bins=30,stacked=True ,density=True,range=[0,200], label=label1)\n",
    "    axes[0,2].hist(Data2['Peak Time'],bins=30,stacked=True ,density=True,range=[0,200],label=label2)\n",
    "    axes[0,2].hist(Data3['Peak Time'],bins=30,stacked=True ,density=True,range=[0,200],label=label3)\n",
    "    axes[0,2].hist(Data4['Peak Time'],bins=30,stacked=True ,density=True,range=[0,200],label=label4)\n",
    "                         \n",
    "                         \n",
    "    axes[1,0].hist(Data1['Peak Amp'],histtype='step',bins=200,stacked=True ,density=True, range=[0,8],label=label1)\n",
    "    axes[1,0].hist(Data2['Peak Amp'],histtype='step',bins=200,stacked=True ,density=True,range=[0,8],label=label2)\n",
    "    axes[1,0].hist(Data3['Peak Amp'],histtype='step',bins=200,stacked=True ,density=True,range=[0,8],label=label3)\n",
    "    axes[1,0].hist(Data4['Peak Amp'],histtype='step',bins=200,stacked=True ,density=True,range=[0,8],label=label4)\n",
    "                         \n",
    "                         \n",
    "    axes[1,1].hist(Data1['25% time'],bins=30,stacked=True ,density=True, range=[0,200],label=label1)\n",
    "    axes[1,1].hist(Data2['25% time'],bins=30,stacked=True ,density=True,range=[0,200],label=label2)\n",
    "    axes[1,1].hist(Data3['25% time'],bins=30,stacked=True ,density=True,range=[0,200],label=label3)\n",
    "    axes[1,1].hist(Data4['25% time'],bins=30,stacked=True ,density=True,range=[0,200],label=label4)\n",
    "    \n",
    "    axes[1,2].hist(Data1['Pulse Time'],bins=100,stacked=True ,density=True,label=label1)\n",
    "    axes[1,2].hist(Data2['Pulse Time'],bins=100,stacked=True ,density=True,label=label2)\n",
    "    axes[1,2].hist(Data3['Pulse Time'],bins=100,stacked=True ,density=True,label=label3)\n",
    "    axes[1,2].hist(Data4['Pulse Time'],bins=100,stacked=True ,density=True,label=label4)\n",
    "                         \n",
    "    axes[2,0].hist(Data1['75% time'],bins=30,stacked=True ,density=True,range=[0,250], label=label1)\n",
    "    axes[2,0].hist(Data2['75% time'],bins=30,stacked=True ,density=True,range=[0,250],label=label2)\n",
    "    axes[2,0].hist(Data3['75% time'],bins=30,stacked=True ,density=True,range=[0,250],label=label3)\n",
    "    axes[2,0].hist(Data4['75% time'],bins=30,stacked=True ,density=True,range=[0,250],label=label4)\n",
    "                         \n",
    "                         \n",
    "    axes[2,1].hist(Data1['50% time'],bins=30,stacked=True ,density=True,range=[0,200], label=label1)\n",
    "    axes[2,1].hist(Data2['50% time'],bins=30,stacked=True ,density=True,range=[0,200],label=label2)\n",
    "    axes[2,1].hist(Data3['50% time'],bins=30,stacked=True ,density=True,range=[0,200],label=label3)\n",
    "    axes[2,1].hist(Data4['50% time'],bins=30,stacked=True ,density=True,range=[0,200],label=label4)\n",
    "\n",
    "    axes[2,2].hist(Data1['time/area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,3], label=label1)\n",
    "    axes[2,2].hist(Data2['time/area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,3],label=label2)\n",
    "    axes[2,2].hist(Data3['time/area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,3],label=label3)\n",
    "    axes[2,2].hist(Data4['time/area'],histtype='step',bins=200,stacked=True ,density=True,range=[0,3],label=label4)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cdda619-ae70-4420-a8e2-9550d069804d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3383616672.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_195909/3383616672.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "########################################CutMethod###################################\n",
    "\n",
    "def cut_metrics(data_guess,Data_real,Data_withtag,decay):\n",
    "\"\"\"\n",
    "Metrics used to determine best cuts\n",
    "\"\"\"\n",
    "        not_guess_data = (Data_withtag.merge(data_guess, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']).drop(columns=['_merge'])   \n",
    "            \n",
    "        \n",
    "        tp=len(data_guess[data_guess['tag'] == decay ])\n",
    "        tn=len(not_guess_data[not_guess_data['tag'] != decay])\n",
    "        fp=len(data_guess[data_guess['tag'] != decay ])\n",
    "        fn=len(not_guess_data[not_guess_data['tag'] == decay])\n",
    "        \n",
    "        \n",
    "            \n",
    "        accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "          \n",
    "        if (tp+fn) == 0:\n",
    "            recall=0 \n",
    "        else:\n",
    "            recall=(tp)/(tp+fn)\n",
    "\n",
    "        precision=(tp)/(tp+fp)\n",
    "        if (recall+precision) == 0:\n",
    "            F=0\n",
    "        else:\n",
    "            F=(2*recall*precision)/(recall+precision)\n",
    "        if (tn+fp) ==0:\n",
    "            specificity=0\n",
    "        else:\n",
    "            specificity=(tn)/(tn+fp)\n",
    "        return accuracy,recall,precision,F,specificity\n",
    "        \n",
    "        \n",
    "def area_cut(start_point,Data_withtag,decay):\n",
    "    \"\"\"\n",
    "    Internal cut function to be looped over\n",
    "    \"\"\"\n",
    "    \n",
    "    area_range_max=np.arange(start_point,500,5)#loops from a given start point through to end\n",
    "    \n",
    "    accuracy_list=[]\n",
    "    recall_list=[]\n",
    "    precision_list=[]\n",
    "    F_list=[]\n",
    "    specificity_list=[]\n",
    "    minval=[]\n",
    "    maxval=[]\n",
    "    for end_point in area_range_max:\n",
    "        \n",
    "            data_guess=Data_withtag[(Data_withtag[\"Pulse Area\"] > start_point) & (Data_withtag[\"Pulse Area\"] < end_point)]    \n",
    "        \n",
    "        \n",
    "            not_guess_data = (Data_withtag.merge(data_guess, how = 'outer' ,indicator=True).loc[lambda x : x['_merge']=='left_only']).drop(columns=['_merge'])   \n",
    "           \n",
    "        \n",
    "            tp=len(data_guess[data_guess['tag'] == decay ])\n",
    "            tn=len(not_guess_data[not_guess_data['tag'] != decay])\n",
    "            fp=len(data_guess[data_guess['tag'] != decay ])\n",
    "            fn=len(not_guess_data[not_guess_data['tag'] == decay])\n",
    "        \n",
    "            \n",
    "            accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "          \n",
    "            if (tp+fn) == 0:\n",
    "                recall=0 \n",
    "            else:\n",
    "                recall=(tp)/(tp+fn)\n",
    "            if (tp+fp) ==0:\n",
    "                precision =0\n",
    "            else:\n",
    "                precision=(tp)/(tp+fp)\n",
    "            if (recall+precision) ==0:\n",
    "                F=0\n",
    "            else:\n",
    "                F=(2*recall*precision)/(recall+precision)\n",
    "            if (tn+fp) ==0:\n",
    "                specificity=0\n",
    "            else:\n",
    "                specificity=(tn)/(tn+fp)\n",
    "            accuracy_list.append(accuracy)\n",
    "            recall_list.append(recall)\n",
    "            precision_list.append(precision)\n",
    "            F_list.append(F)\n",
    "            specificity_list.append(specificity)\n",
    "            minval.append(start_point)\n",
    "            maxval.append(end_point)\n",
    "    return [minval,maxval,accuracy_list,recall_list,precision_list,F_list,specificity_list]\n",
    "\n",
    "def auto_cut(set1,set2,set3,set4,combined,decay1,decay2,decay3,decay4):\n",
    "\"\"\"\n",
    "Cut method implimentation\n",
    "\"\"\"\n",
    "    columns=[\"Pulse Area\",\"Coincidence\", \"Peak Time\", \"Peak Amp\", \"25% time\",'Pulse Time', \"75% time\", \"50% time\",\"time/area\"]\n",
    "    info_columns={'minval' : [],'maxval' : [],'accuracy' : [], 'recall' : [], 'precision' : [], 'F1 score' : [], 'specificity' :[]}\n",
    "    \n",
    "    area_range_min=np.arange(0,500,5)\n",
    "    \n",
    "    with mp.Pool(40) as pool:#parallel processing of specific cuts\n",
    "         set1_data=list(tqdm.tqdm(pool.imap(functools.partial(area_cut,Data_withtag=combined,decay=decay1),area_range_min),total=100))\n",
    "    info_set1=pd.DataFrame(np.concatenate(set1_data,axis=1).T.tolist(),columns=info_columns)\n",
    "           \n",
    "    \n",
    "    with mp.Pool(40) as pool:\n",
    "         set2_data=list(tqdm.tqdm(pool.imap(functools.partial(area_cut,Data_withtag=combined,decay=decay2),area_range_min),total=100))\n",
    "    info_set2=pd.DataFrame(np.concatenate(set2_data,axis=1).T.tolist(),columns=info_columns)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with mp.Pool(40) as pool:\n",
    "         set3_data=list(tqdm.tqdm(pool.imap(functools.partial(area_cut,Data_withtag=combined,decay=decay3),area_range_min),total=100))\n",
    "    info_set3=pd.DataFrame(np.concatenate(set3_data,axis=1).T.tolist(),columns=info_columns)       \n",
    "    \n",
    "    \n",
    "    \n",
    "    with mp.Pool(40) as pool:\n",
    "         set4_data=list(tqdm.tqdm(pool.imap(functools.partial(area_cut,Data_withtag=combined,decay=decay4),area_range_min),total=100))\n",
    "    info_set4=pd.DataFrame(np.concatenate(set4_data,axis=1).T.tolist(),columns=info_columns)\n",
    "    \n",
    "    #best cut is determined by F1 score\n",
    "    max_set1=info_set1[info_set1['F1 score'] == info_set1['F1 score'].max()]\n",
    "    max_set2=info_set2[info_set2['F1 score'] == info_set2['F1 score'].max()]\n",
    "    max_set3=info_set3[info_set3['F1 score'] == info_set3['F1 score'].max()]\n",
    "    max_set4=info_set4[info_set4['F1 score'] == info_set4['F1 score'].max()]       \n",
    "    \n",
    "    info=[info_set1,info_set2,info_set3,info_set4]\n",
    "\n",
    "    #Cut with best F1 score is then selected   \n",
    "   \n",
    "    set1_data_guess=combined[(combined[\"Pulse Area\"] > max_set1['minval'].max()) & (combined[\"Pulse Area\"] < max_set1['maxval'].max())]\n",
    "    set2_data_guess=combined[(combined[\"Pulse Area\"] > max_set2['minval'].max()) & (combined[\"Pulse Area\"] < max_set2['maxval'].max())]\n",
    "    set3_data_guess=combined[(combined[\"Pulse Area\"] > max_set3['minval'].max()) & (combined[\"Pulse Area\"] < max_set3['maxval'].max())]\n",
    "    set4_data_guess=combined[((combined[\"Pulse Area\"] > max_set4['minval'].max()) & (combined[\"Pulse Area\"] < max_set4['maxval'].max()))]\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    #metrics for this new cut is then determined\n",
    "    set1_accuracy,set1_recall,set1_precision,set1_F,set1_specificity=cut_metrics(set1_data_guess,set1,combined,decay1)\n",
    "    set2_accuracy,set2_recall,set2_precision,set2_F,set2_specificity=cut_metrics(set2_data_guess,set2,combined,decay2)\n",
    "    set3_accuracy,set3_recall,set3_precision,set3_F,set3_specificity=cut_metrics(set3_data_guess,set3,combined,decay3)\n",
    "    set4_accuracy,set4_recall,set4_precision,set4_F,set4_specificity=cut_metrics(set4_data_guess,set4,combined,decay4)\n",
    "    data=[set1_data_guess,set2_data_guess,set3_data_guess,set4_data_guess]\n",
    "    #and added to array\n",
    "    columns={'sample' : [], 'accuracy' : [], 'recall' : [], 'precision' : [], 'F1 score' : [], 'specificity' : []}\n",
    "    metrics=pd.DataFrame(columns=columns)\n",
    "    metrics['sample']=[decay1,decay2,decay3,decay4]\n",
    "    metrics['accuracy']=[set1_accuracy,set2_accuracy,set3_accuracy,set4_accuracy]\n",
    "    metrics['recall']=[set1_recall,set2_recall,set3_recall,set4_recall]\n",
    "    metrics['precision']=[set1_precision,set2_precision,set3_precision,set4_precision]\n",
    "    metrics['F1 score']=[set1_F,set2_F,set3_F,set4_F]\n",
    "    metrics['specificity']=[set1_specificity,set2_specificity,set3_specificity,set4_specificity]\n",
    "    print(metrics)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return metrics,data,info\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e4dad7-bb46-40d5-a90a-dbc84d1ac82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################Statistical method################################\n",
    "\n",
    "def PDE(decaytype,background):\n",
    "    \"\"\"\n",
    "    generates likelihood histogram\n",
    "    \"\"\"\n",
    "    bins = np.linspace(0,200,1000)\n",
    "    \n",
    "    signal = np.histogram(decaytype['Pulse Area'], bins, density=False)#determines signal\n",
    "    signal = list(signal)\n",
    "    signal[0] = [i/len(electrons['Pulse Area']) for i in signal[0]]\n",
    "    \n",
    "    drop = decaytype['tag']\n",
    "    dropvalues = ['{}'.format(drop)]\n",
    "\n",
    "\n",
    "\n",
    "    background = background[background.tag.isin(dropvalues) == False]#determines background\n",
    "    background = np.histogram(background['Pulse Area'],bins, density=False)\n",
    "    background = list(background)\n",
    "    background[0] = [i/len(decaytype['Pulse Area']) for i in background[0]]\n",
    "    \n",
    "    L = np.divide(signal[0],(np.add(signal[0] , background[0])))#finds likelihood\n",
    "    centre = [ (y+x)/2 for x, y in zip(signal[1],signal[1][1:]) ] \n",
    "    \n",
    "    return L,centre \n",
    "\n",
    "def NNE(input_number,input_array,outputarray):\n",
    "    \"\"\"\n",
    "    maps new values to generated histogram to assign likelihood\n",
    "    \"\"\"\n",
    "    \n",
    "    mapped = list(zip(input_array,outputarray))\n",
    "    NN = min(input_array, key=lambda x:abs(x-input_number))\n",
    "    index = input_array.index(NN)\n",
    "    output = mapped[index][1]\n",
    "    \n",
    "    return output\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def pde_run(electrons,alphas,neutron_data,rockgamma_data,decay_type):\n",
    "    \"\"\"\n",
    "    runs pde functions and plots discriminators\n",
    "    \"\"\"\n",
    "    EL,Ecentre=PDE(electrons,decay_type)\n",
    "    AL,Acentre=PDE(alphas,decay_type)\n",
    "    NL,Ncentre=PDE(neutron_data,decay_type)\n",
    "    GL,Gcentre=PDE(rockgamma_data,decay_type)\n",
    "           \n",
    "\n",
    "                  \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(Ecentre,EL,label='electrons',color='c')\n",
    "    plt.plot(Acentre,AL,label='alpha',color='y')\n",
    "    plt.plot(Ncentre,NL,label='neutrons',color='r')\n",
    "    plt.plot(Gcentre,GL,label='RockGamma',color='b')\n",
    "\n",
    "    plt.xlabel(\"Pulse Area\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(\"Probability Distribution\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    pde_df=decay_type.copy()\n",
    "    \n",
    "    pde_df.drop(pde_df.columns.difference(['Pulse Area','tag']), 1, inplace=True)\n",
    "\n",
    "\n",
    "    pde_df['alpha prob'] = tqdm.tqdm(pde_df.apply(lambda row: NNE(row['Pulse Area'],Acentre,AL),axis=1),total=708046)\n",
    "    pde_df['electron prob'] = tqdm.tqdm(pde_df.apply(lambda row: NNE(row['Pulse Area'],Ecentre,EL),axis=1),total=708046)\n",
    "    pde_df['gamma prob'] = tqdm.tqdm(pde_df.apply(lambda row: NNE(row['Pulse Area'],Gcentre,GL),axis=1),total=708046)\n",
    "    pde_df['neutron prob'] = tqdm.tqdm(pde_df.apply(lambda row: NNE(row['Pulse Area'],Ncentre,NL),axis=1),total=708046)\n",
    "\n",
    "\n",
    "    one_hot = pd.get_dummies(pde_df['tag'])\n",
    "    pde_df=pd.concat([pde_df, one_hot], axis=1)\n",
    "\n",
    "    categories = ['alpha','electron','RockGamma','neutron']\n",
    "\n",
    "    Afpr, Atpr, Athresholds = roc_curve(pde_df['alpha'], pde_df['alpha prob'])\n",
    "    Bfpr, Btpr, Bthresholds = roc_curve(pde_df['electron'], pde_df['electron prob'])\n",
    "    Nfpr, Ntpr, Nthresholds = roc_curve(pde_df['neutron'], pde_df['neutron prob'])\n",
    "    Rfpr, Rtpr, Rthresholds = roc_curve(pde_df['RockGamma'], pde_df['gamma prob'])\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.plot(Afpr,Atpr,label='alpha vs background, AUC score:{:.3f} '.format(auc(Afpr,Atpr)), color = 'c')\n",
    "    plt.plot(Bfpr,Btpr,label='electron vs background, AUC score:{:.3f}' .format(auc(Bfpr,Btpr)),color = 'y')\n",
    "    plt.plot(Nfpr,Ntpr,label='neutron vs background, AUC score:{:.3f} ' .format(auc(Nfpr,Ntpr)),color = 'r')\n",
    "    plt.plot(Rfpr,Rtpr,label='rockgamma vs background, AUC score:{:.3f} ' .format(auc(Rfpr,Rtpr)),color = 'b')\n",
    "    plt.plot([0,1],[0,1], linestyle = '--', label = 'No classifier', color = 'm')\n",
    "\n",
    "\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    true_labels=np.asarray([pde_df['alpha'],pde_df['electron'],pde_df['RockGamma'],pde_df['neutron']])\n",
    "    predicted_labels=np.asarray([pde_df['alpha prob'],pde_df['electron prob'],pde_df['gamma prob'],pde_df['neutron prob']])\n",
    "    pde_metrics_list=[]\n",
    "    for i in range (4):\n",
    "        confusion=confusion_matrix(true_labels[i], discriminator(predicted_labels[i],0.2))\n",
    "        pde_metrics=confusion_metrics(confusion)\n",
    "        pde_metrics_list.append(pde_metrics)\n",
    "    \n",
    "    columns={'accuracy' : [], 'recall' : [], 'precision' : [], 'F1 score' : [], 'specificity' : []}\n",
    "    pde_metrics=pd.DataFrame(np.asarray(pde_metrics_list),columns=columns)    \n",
    "    pde_metrics['method']='Statistical'\n",
    "\n",
    "    pde_metrics['sample']=categories   \n",
    "    cm=confusion_matrix(true_labels.T.argmax(axis=1), predicted_labels.T.argmax(axis=1),normalize='true')\n",
    "\n",
    "\n",
    "    #sns.set(font_scale=1.4)\n",
    "    sns.heatmap(cm,xticklabels=categories,yticklabels=categories, annot=True,fmt='.2%',cbar=False, cmap='Blues')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    sns.histplot(ax=ax[0,0],data=pde_df, x=\"alpha prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    ax[0,0].set_title('Alpha Discriminator')\n",
    "    sns.histplot(ax=ax[0,1],data=pde_df, x=\"electron prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    ax[0,1].set_title('Electron Discriminator')\n",
    "    sns.histplot(ax=ax[1,0],data=pde_df, x=\"neutron prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    ax[1,0].set_title('Neutron Discriminator')\n",
    "    sns.histplot(ax=ax[1,1],data=pde_df, x=\"gamma prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    ax[0,0].set_title('Gamma Discriminator')\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.histplot(data=pde_df, x=\"alpha prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    plt.xlabel('Alpha Probability')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.histplot(data=pde_df, x=\"electron prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    plt.xlabel('Electron Probability')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.histplot(data=pde_df, x=\"gamma prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    plt.xlabel('Gamma Probability')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.figure(figsize=(10,10))\n",
    "    sns.histplot(data=pde_df, x=\"neutron prob\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "    plt.xlabel('Neutron Probability')\n",
    "    plt.show()\n",
    "    electron_metrics=confusion_metrics(cm_electron)\n",
    "    alpha_metrics=confusion_metrics(cm_alpha)\n",
    "    neutron_metrics=confusion_metrics(cm_neutron)\n",
    "    gamma_metrics=confusion_metrics(cm_gamma)\n",
    "    columns={'accuracy' : [], 'recall' : [], 'precision' : [], 'F1 score' : [], 'specificity' : []}\n",
    "    pde_metrics=pd.DataFrame([electron_metrics,alpha_metrics,neutron_metrics,gamma_metrics],columns=columns)\n",
    "    return pde_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc7213c-e019-4476-8615-6e894cae6363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##################################NeuralNetwork#############################\n",
    "\n",
    "def binary_classifier(output_bias):\n",
    "    \"\"\"\n",
    "    Binary classifier model\n",
    "    \"\"\"\n",
    "    output_bias=tf.keras.initializers.Constant(output_bias)\n",
    "\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(9,input_dim=9, activation='relu'),\n",
    "    keras.layers.Dense(18, activation=\"relu\"),\n",
    "    keras.layers.Dense(18, activation=\"relu\"),\n",
    "    keras.layers.Dense(18, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\",bias_initializer=output_bias)])\n",
    "    \n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.005)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',  metrics=['AUC',\"binary_accuracy\"], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "def multiclassifier():\n",
    "    \"\"\"\n",
    "    Multiclass model\n",
    "    \"\"\"\n",
    "    \n",
    "    output_bias=tf.keras.initializers.Constant()\n",
    "\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "    keras.layers.Dense(9,input_dim=9, activation='relu'),\n",
    "    keras.layers.Dense(18, activation=\"relu\"),\n",
    "    keras.layers.Dense(18, activation=\"relu\"),\n",
    "    keras.layers.Dense(18, activation=\"relu\"),\n",
    "    keras.layers.Dense(4, activation=\"softmax\")])\n",
    "    \n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate = 0.005)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy',  metrics=['AUC',\"categorical_accuracy\"], optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(Data_withtag,sample1,sample2):\n",
    "    \"\"\"\n",
    "    PreProcessing for binary classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    features=Data_withtag[(Data_withtag['tag'] == sample1) | (Data_withtag['tag'] == sample2)]\n",
    "    labels=Data_withtag[(Data_withtag['tag'] == sample1) | (Data_withtag['tag'] == sample2)]\n",
    "    features=(features.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "\n",
    "    features=tf.keras.utils.normalize(features).squeeze()\n",
    "    \n",
    "    labels=labels.iloc[:,-1]\n",
    "    labels = pd.get_dummies(data = labels, columns=[sample1],drop_first=True)\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "    labels_train=np.asarray(labels_train).astype('float32').reshape((-1,1))\n",
    "    labels_test=np.asarray(labels_test).astype('float32').reshape((-1,1))\n",
    "    return features_train,features_test,labels_train,labels_test\n",
    "\n",
    "def confusion_metrics(confusion_matrix):\n",
    "    \"\"\"\n",
    "    Function to generate confusion matrix metrics\n",
    "    \"\"\"\n",
    "    tp=confusion_matrix[0,0]\n",
    "    fn=confusion_matrix[0,1]\n",
    "    fp=confusion_matrix[1,0]\n",
    "    tn=confusion_matrix[1,1]\n",
    "    \n",
    "    accuracy=(tp+tn)/(tp+fn+fp+tn)\n",
    "    recall=(tp)/(tp+fn)\n",
    "    precision=(tp)/(tp+fp)\n",
    "    F1=(2*recall*precision)/(recall+precision)\n",
    "    specificity=(tn)/(tn+fp)\n",
    "\n",
    "    \n",
    "\n",
    "    return accuracy,recall,precision,F1,specificity\n",
    "\n",
    "def discriminator(probability,threshold):\n",
    "    \"\"\"\n",
    "    discriminator for a given threshold\n",
    "    \"\"\"\n",
    "    prediction=np.where(probability > threshold, 1, 0)\n",
    "    return prediction\n",
    "\n",
    "def binary_run(Data_withtag,sample1,sample2):\n",
    "    \"\"\"\n",
    "    Function that processes and runs the binary model\n",
    "    \"\"\"\n",
    "    \n",
    "    stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=40)#callback to monitor progress\n",
    "    features_train,features_test,labels_train,labels_test=preprocess(Data_withtag,sample1,sample2)\n",
    "    train_label_df=pd.DataFrame(labels_train,columns=['tag'])\n",
    "    pos=len(train_label_df[train_label_df['tag']==1])\n",
    "    neg=len(train_label_df[train_label_df['tag']==0])\n",
    "    total=len(train_label_df['tag'])\n",
    "    initial_bias = np.log([pos/neg])#custom weights set\n",
    "    weight_for_0 = (1 / neg) * (total / 2.0)\n",
    "    weight_for_1 = (1 / pos) * (total / 2.0)\n",
    "\n",
    "    class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "    \n",
    "    model=binary_classifier(output_bias=initial_bias)\n",
    "\n",
    "    model.summary()\n",
    "    history=model.fit(features_train, labels_train, epochs=400, batch_size=1024, verbose=1,validation_split=0.2, callbacks=[stop],class_weight=class_weight)\n",
    "\n",
    "    proba = model.predict(features_test)\n",
    "\n",
    "    \n",
    "    return history,labels_test,proba\n",
    "\n",
    "def multiclass(decay_type):\n",
    "    \"\"\"\n",
    "    Processing and running of multiclass model\n",
    "    \"\"\"\n",
    "\n",
    "    oversample = SMOTE()#oversampling to balence dataset\n",
    "    stop = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=200)\n",
    "    features=(decay_type.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "    features=tf.keras.utils.normalize(features).squeeze()\n",
    "    labels=decay_type.iloc[:,-1]\n",
    "    labels = pd.get_dummies(data = labels)\n",
    "    features, labels= oversample.fit_resample(np.asarray(features), np.asarray(labels))\n",
    "\n",
    "    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)\n",
    "    labels_train=np.asarray(labels_train).astype('float32')\n",
    "    labels_test=np.asarray(labels_test).astype('float32')\n",
    "\n",
    "    model=multiclassifier()\n",
    "\n",
    "    model.summary()\n",
    "    history=model.fit(features_train, labels_train, epochs=400, batch_size=1024, verbose=1,validation_split=0.2, callbacks=[stop])\n",
    "    proba = model.predict(features_test)\n",
    "    return history, labels_test ,proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ca01190-1750-45fc-aec3-649e5ed04343",
   "metadata": {},
   "outputs": [],
   "source": [
    "internals_data,neutron_data,rockgamma_data=data()\n",
    "Th232_chain,U238_chain,reminder, electrons, alphas,reminder_type=separate(internals_data,neutron_data,rockgamma_data)\n",
    "decay_type=pd.concat([electrons,alphas,neutron_data,rockgamma_data],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4ee533-7efb-4cdd-8882-b3d7f5528b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############Additional Preprocess###################\n",
    "info_df=(decay_type.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "info_df=tf.keras.utils.normalize(info_df).squeeze()\n",
    "\n",
    "electron_df=(electrons.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "electron_df=tf.keras.utils.normalize(electron_df).squeeze()\n",
    "electron_df['tag']='electron'\n",
    "electron_df['tag'] = electron_df['tag'].astype(\"string\")\n",
    "alpha_df=(alphas.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "alpha_df=tf.keras.utils.normalize(alpha_df).squeeze()\n",
    "alpha_df['tag']='alpha'\n",
    "alpha_df['tag'] = alpha_df['tag'].astype(\"string\")\n",
    "neutron_df=(neutron_data.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "neutron_df=tf.keras.utils.normalize(neutron_df).squeeze()\n",
    "neutron_df['tag']='neutron'\n",
    "neutron_df['tag'] = neutron_df['tag'].astype(\"string\")\n",
    "gamma_df=(rockgamma_data.iloc[:,0:9]).apply(pd.to_numeric)\n",
    "gamma_df=tf.keras.utils.normalize(gamma_df).squeeze()\n",
    "gamma_df['tag']='gamma'\n",
    "gamma_df['tag'] = gamma_df['tag'].astype(\"string\")\n",
    "decay_df=pd.concat([electron_df,alpha_df,neutron_df,gamma_df],ignore_index=True)\n",
    "\n",
    "###############Some seaborn plots#####################\n",
    "\n",
    "#plt.figure(figsize=(10,10))\n",
    "#plt.title(\"Boxplot of Pulse Length for combined dataset\")\n",
    "#sns.boxplot(x='tag',y='time/area',data=decay_df)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(10,10))\n",
    "#sns.set_style('whitegrid')\n",
    "#sns.violinplot(x='tag',y='Pulse Area',data=chains)\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#plt.title(\"pairplot for combined dataset\")\n",
    "#sns.set_style('whitegrid')\n",
    "#sns.pairplot(decay_type,kind=\"scatter\", hue='tag', height=3,    \n",
    "#    x_vars=[\"Pulse Area\", \"Coincidence\",\"Peak Time\",\"time/area\",'Pulse Time'],\n",
    "#    y_vars=[\"Pulse Area\", \"Coincidence\",\"Peak Time\",'time/area','Pulse Time'],\n",
    "#    plot_kws=dict(s=80, edgecolor=\"white\", linewidth=2.5, alpha=0.3)\n",
    "#)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e102f18-8dcf-4adc-b7fb-fa5e0f936119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot(electrons,alphas,neutron_data,rockgamma_data,decay_type,'electrons','alphas','neutrons','RockGamma')\n",
    "\n",
    "cut_metrics,cut_df,info=auto_cut(electrons,alphas,neutron_data,rockgamma_data,decay_type,'electron','alpha','neutron','RockGamma')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd056793-443e-4685-ba90-36467feb33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_metrics=pde_run(electrons,alphas,neutron_data,rockgamma_data,decay_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14461e9-955e-49cb-8a2b-b0b54eddf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################Visualisation for the cut method\n",
    "\n",
    "data1=(pd.DataFrame(cut_df[0]))\n",
    "data1['cut tag']='electrons'\n",
    "data2=(pd.DataFrame(cut_df[1]))\n",
    "data2['cut tag']='alpha'\n",
    "data3=(pd.DataFrame(cut_df[2]))\n",
    "data3['cut tag']='neutron'\n",
    "data4=(pd.DataFrame(cut_df[3]))\n",
    "data4['cut tag']='RockGamma'\n",
    "\n",
    "cut_df=pd.concat([data1,data2,data3,data4],ignore_index=True)\n",
    "\n",
    "cut_metrics['method']='cut'\n",
    "\n",
    "plt.close()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.histplot(data=cut_df, x=\"Pulse Area\",hue='tag', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "plt.xlim(0,500)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "label_true =pd.get_dummies(data = cut_df['tag'])\n",
    "\n",
    "label_pred =pd.get_dummies(data = cut_df['cut tag'])\n",
    "cm=confusion_matrix(np.asarray(label_true).argmax(axis=1), np.asarray(label_pred).argmax(axis=1),normalize='pred')\n",
    "\n",
    "categories = ['electrons','alpha','RockGamma','neutron']\n",
    "\n",
    "sns.heatmap(cm,xticklabels=categories,yticklabels=categories, annot=True,fmt='.2%',cbar=False, cmap='Blues')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac3e2f-ee8a-4e3f-a10c-282c5a587fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains=pd.concat([Th232_chain,U238_chain],ignore_index=True)\n",
    "NA_history,NA_labels_test,NA_proba=binary_run(decay_type,\"neutron\",\"RockGamma\")\n",
    "EA_history,EA_labels_test,EA_proba=binary_run(decay_type,\"electron\",\"alpha\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b91362-b5fb-4740-be5f-931abb5e8bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################visualisation for binary classifier##########################  \n",
    "NA_fpr, NA_tpr, NA_thresh = roc_curve(NA_labels_test, NA_proba, pos_label=1)\n",
    "NA_prec, NA_recall,NA_recall_thresh = precision_recall_curve(NA_labels_test, NA_proba, pos_label=1)\n",
    "NA_predictions = discriminator(NA_proba,0.5)\n",
    "NA_confusion=confusion_matrix(NA_labels_test, NA_predictions)\n",
    "\n",
    "NA_F1=(2*NA_recall*NA_prec)/(NA_recall+NA_prec)\n",
    "\n",
    "EA_fpr, EA_tpr, EA_thresh = roc_curve(EA_labels_test, EA_proba, pos_label=1)\n",
    "EA_prec, EA_recall,EA_recall_thresh = precision_recall_curve(EA_labels_test, EA_proba, pos_label=1)\n",
    "EA_predictions = discriminator(EA_proba,0.5)\n",
    "EA_confusion=confusion_matrix(EA_labels_test, EA_predictions)\n",
    "\n",
    "EA_F1=(2*EA_recall*EA_prec)/(EA_recall+EA_prec)\n",
    "\n",
    "\n",
    "\n",
    "plt.close()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "axes[0].plot(NA_fpr, NA_tpr,color='orange', label='Alpha and Neutron (area = {:.3f})'.format(auc(NA_fpr,NA_tpr)))\n",
    "axes[0].plot(EA_fpr, EA_tpr,color='green', label='Electron and rockgamma (area = {:.3f})'.format(auc(EA_fpr,EA_tpr)))\n",
    "axes[0].plot([0, 1], [0, 1],linestyle='dashdot',color='red', label='No classifier')\n",
    "axes[0].set_title('Binary ROC curve')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive rate')\n",
    "axes[0].legend(loc='best')\n",
    "\n",
    "axes[1].plot(NA_recall, NA_prec,color='orange', label='Alpha and Neutron (area = {:.3f})'.format(auc(NA_recall,NA_prec)))\n",
    "axes[1].plot(EA_recall, EA_prec,color='green', label='Electron and rockgamma (area = {:.3f})'.format(auc(EA_recall,EA_prec)))\n",
    "axes[1].set_title('Binary precision recall curve')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e93439-b34f-44f5-b38e-476416996f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning history for binary classifer\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "fig.tight_layout()\n",
    "axes[0,0].plot(NA_history.history['loss'],label='Loss')\n",
    "axes[0,0].plot(NA_history.history['val_loss'],label='Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "\n",
    "axes[0,0].legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "axes[0,1].plot(NA_history.history['auc'],label='auc')\n",
    "axes[0,1].plot(NA_history.history['val_auc'],label='Validation auc')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('AUC')\n",
    "axes[0,1].legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "axes[0,2].plot(NA_history.history['binary_accuracy'],label='binary accuracy')\n",
    "axes[0,2].plot(NA_history.history['val_binary_accuracy'],label='binary accuracy')\n",
    "axes[0,2].set_xlabel('Epoch')\n",
    "axes[0,2].set_ylabel('binary accuracy')\n",
    "axes[0,2].legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "axes[1,0].plot(EA_history.history['loss'],label='Loss',color='green')\n",
    "axes[1,0].plot(EA_history.history['val_loss'],label='Validation Loss',color='red')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Loss')\n",
    "axes[1,0].legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "axes[1,1].plot(EA_history.history['auc'],label='auc',color='green')\n",
    "axes[1,1].plot(EA_history.history['val_auc'],label='Validation auc',color='red')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('AUC')\n",
    "axes[1,1].legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "axes[1,2].plot(EA_history.history['binary_accuracy'],label='binary accuracy',color='green')\n",
    "axes[1,2].plot(EA_history.history['val_binary_accuracy'],label='binary accuracy',color='red')\n",
    "axes[1,2].set_xlabel('Epoch')\n",
    "axes[1,2].set_ylabel('binary accuracy')\n",
    "axes[1,2].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f2e882-37e8-4386-878f-5e447da2f502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrices for binary classifier\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "NA_cm=confusion_matrix(NA_labels_test,discriminator(NA_proba,0.5),normalize='pred')\n",
    "categories = ['neutron','alpha']\n",
    "\n",
    "sns.heatmap(ax=axes[0],data=NA_cm,xticklabels=categories,yticklabels=categories, annot=True,fmt='.2%',cbar=False, cmap='Blues')\n",
    "\n",
    "EA_cm=confusion_matrix(EA_labels_test,discriminator(EA_proba,0.5),normalize='pred')\n",
    "categories = ['electron','RockGamma']\n",
    "sns.heatmap(ax=axes[1],data=EA_cm,xticklabels=categories,yticklabels=categories, annot=True,fmt='.2%',cbar=False, cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "NA_metrics=confusion_metrics(NA_confusion)\n",
    "EA_metrics=confusion_metrics(EA_confusion)\n",
    "columns={'accuracy' : [], 'recall' : [], 'precision' : [], 'F1 score' : [], 'specificity' : []}\n",
    "BN_metrics=pd.DataFrame(np.asarray([NA_metrics,EA_metrics]),columns=columns)\n",
    "print(BN_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf31d5a-2456-413f-beb8-a6d843648e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminators for binary classifier\n",
    "prob_col=['prob','true']\n",
    "NA_data=pd.DataFrame([],columns=prob_col)\n",
    "NA_data['prob']=NA_proba.tolist()\n",
    "NA_data['true']=NA_labels_test.tolist()\n",
    "NA_data=NA_data.explode(column=prob_col)\n",
    "NA_data.loc[NA_data['true'] ==1, 'pred'] = 'alpha'\n",
    "NA_data.loc[NA_data['true'] ==0, 'pred'] = 'neutron'\n",
    "\n",
    "\n",
    "prob_col=['prob','true']\n",
    "EA_data=pd.DataFrame([],columns=prob_col)\n",
    "EA_data['prob']=EA_proba.tolist()\n",
    "EA_data['true']=EA_labels_test.tolist()\n",
    "EA_data=EA_data.explode(column=prob_col)\n",
    "EA_data.loc[EA_data['true'] ==1, 'pred'] = 'RockGamma'\n",
    "EA_data.loc[EA_data['true'] ==0, 'pred'] = 'electron'\n",
    "plt.close()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 8))\n",
    "\n",
    "sns.histplot(ax=axes[0],data=NA_data, x=\"prob\",hue='pred', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "axes[0].set_xlabel('Probability')\n",
    "axes[0].set_title('Neutron and Alpha discriminator')\n",
    "sns.histplot(ax=axes[1],data=EA_data, x=\"prob\",hue='pred', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "axes[1].set_xlabel('Probability')\n",
    "axes[1].set_title('Electron and rockgamma discriminator')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8708809-f142-4bb4-81fa-6c5549a50829",
   "metadata": {},
   "outputs": [],
   "source": [
    "history, labels_test,proba=multiclass(decay_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ccd45-6aaf-47d5-948b-27a13858eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation for multiclass\n",
    "fpr = {}\n",
    "tpr = {}\n",
    "thresh ={}\n",
    "prec={}\n",
    "recall={}\n",
    "recall_thresh={}\n",
    "confusion_list=[]\n",
    "NN_metrics_list=[]\n",
    "F1={}\n",
    "n_class = 4\n",
    "sample=[\"electron\",\"alpha\",'Gamma','neutron']\n",
    "for i in range(n_class):    \n",
    "    fpr[i], tpr[i], thresh[i] = roc_curve(labels_test[:,i], proba[:,i], pos_label=1)\n",
    "    prec[i], recall[i],recall_thresh[i] = precision_recall_curve(labels_test[:,i], proba[:,i], pos_label=1)\n",
    "    predictions = discriminator(proba[:,i],0.5)\n",
    "    confusion=confusion_matrix(labels_test[:,i], predictions)\n",
    "    NN_metrics_list.append(confusion_metrics(confusion))\n",
    "    F1[i]=(2*recall[i]*prec[i])/(recall[i]+prec[i])\n",
    "\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax[0].plot(fpr[0], tpr[0],color='orange', label='Electrons (area = {:.3f})'.format(auc(fpr[0],tpr[0])))\n",
    "ax[0].plot(fpr[1], tpr[1],color='green', label='Alphas (area = {:.3f})'.format(auc(fpr[1],tpr[1])))\n",
    "ax[0].plot(fpr[2], tpr[2],color='blue', label='Gamma (area = {:.3f})'.format(auc(fpr[2],tpr[2])))\n",
    "ax[0].plot(fpr[3], tpr[3],color='yellow', label='Neutrons (area = {:.3f})'.format(auc(fpr[3],tpr[3])))\n",
    "ax[0].plot([0, 1], [0, 1],linestyle='dashdot',color='red', label='No classifier')\n",
    "ax[0].set_title('Multiclass ROC curve')\n",
    "ax[0].set_xlabel('False Positive Rate')\n",
    "ax[0].set_ylabel('True Positive rate')\n",
    "ax[0].legend(loc='best')\n",
    "\n",
    "\n",
    "\n",
    "ax[1].plot(recall[0], prec[0],color='orange', label='Electrons (area = {:.3f})'.format(auc(recall[0],prec[0])))\n",
    "ax[1].plot(recall[1], prec[1],color='green', label='Alphas (area = {:.3f})'.format(auc(recall[1],prec[1])))\n",
    "ax[1].plot(recall[2], prec[2],color='blue', label='Gamma (area = {:.3f})'.format(auc(recall[2],prec[2])))\n",
    "ax[1].plot(recall[3], prec[3],color='yellow', label='Neutrons (area = {:.3f})'.format(auc(recall[3],prec[3])))\n",
    "ax[1].set_title('Multiclass precision recall curve')\n",
    "ax[1].set_xlabel('Recall')\n",
    "ax[1].set_ylabel('Precisioy')\n",
    "ax[1].legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0feacf0-076b-4023-b9aa-894d3644f14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning history for multiclass\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 6))\n",
    "ax[0].plot(history.history['loss'],label='Loss')\n",
    "ax[0].plot(history.history['val_loss'],label='Validation Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title(\"Multiclassifier loss\")\n",
    "\n",
    "\n",
    "\n",
    "ax[1].plot(history.history['auc'],label='auc')\n",
    "ax[1].plot(history.history['val_auc'],label='Validation auc')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_ylabel('AUC')\n",
    "ax[1].legend(loc='best')\n",
    "ax[1].set_title(\"Multiclassifier area under curve\")\n",
    "\n",
    "\n",
    "\n",
    "ax[2].plot(history.history['categorical_accuracy'],label='categorical accuracy')\n",
    "ax[2].plot(history.history['val_categorical_accuracy'],label='Validation categorical accuracy')\n",
    "ax[2].set_xlabel('Epoch')\n",
    "ax[2].set_ylabel('categorical accuracy')\n",
    "ax[2].legend(loc='best')\n",
    "ax[2].set_title(\"Multiclassifier categorical accuracy\")\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5be41-e1a7-44a3-8011-51ab859c36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix for multiclass\n",
    "cm=confusion_matrix(np.asarray(labels_test).argmax(axis=1), np.asarray(proba).argmax(axis=1),normalize='true')\n",
    "\n",
    "categories = ['electrons','alpha','gamma','neutrons']\n",
    "#cmn=cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "sns.heatmap(cm,xticklabels=categories,yticklabels=categories, annot=True,fmt='.2%',cbar=False, cmap='Blues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138c783-0bad-4940-9a4c-29e27e68234a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminators for multiclass\n",
    "\n",
    "prob_col=['prob','electron_true','alpha_true','gamma_true','neutron_true']\n",
    "\n",
    "\n",
    "electron_data=pd.DataFrame([],columns=prob_col)\n",
    "electron_data['prob']=(proba[:,0])\n",
    "electron_data['electron_true']=(labels_test[:,0])\n",
    "electron_data['alpha_true']=(labels_test[:,1])\n",
    "electron_data['gamma_true']=(labels_test[:,2])\n",
    "electron_data['neutron_true']=(labels_test[:,3])\n",
    "electron_data.loc[electron_data['electron_true'] ==1, 'pred'] = 'electron'\n",
    "electron_data.loc[electron_data['alpha_true'] ==1, 'pred'] = 'alpha'\n",
    "electron_data.loc[electron_data['gamma_true'] ==1, 'pred'] = 'gamma'\n",
    "electron_data.loc[electron_data['neutron_true'] ==1, 'pred'] = 'neutron'\n",
    "\n",
    "\n",
    "alpha_data=pd.DataFrame([],columns=prob_col)\n",
    "alpha_data['prob']=(proba[:,1])\n",
    "alpha_data['electron_true']=(labels_test[:,0])\n",
    "alpha_data['alpha_true']=(labels_test[:,1])\n",
    "alpha_data['gamma_true']=(labels_test[:,2])\n",
    "alpha_data['neutron_true']=(labels_test[:,3])\n",
    "alpha_data.loc[alpha_data['electron_true'] ==1, 'pred'] = 'electron'\n",
    "alpha_data.loc[alpha_data['alpha_true'] ==1, 'pred'] = 'alpha'\n",
    "alpha_data.loc[alpha_data['gamma_true'] ==1, 'pred'] = 'gamma'\n",
    "alpha_data.loc[alpha_data['neutron_true'] ==1, 'pred'] = 'neutron'\n",
    "\n",
    "gamma_data=pd.DataFrame([],columns=prob_col)\n",
    "gamma_data['prob']=(proba[:,2])\n",
    "gamma_data['electron_true']=(labels_test[:,0])\n",
    "gamma_data['alpha_true']=(labels_test[:,1])\n",
    "gamma_data['gamma_true']=(labels_test[:,2])\n",
    "gamma_data['neutron_true']=(labels_test[:,3])\n",
    "gamma_data.loc[gamma_data['electron_true'] ==1, 'pred'] = 'electron'\n",
    "gamma_data.loc[gamma_data['alpha_true'] ==1, 'pred'] = 'alpha'\n",
    "gamma_data.loc[gamma_data['gamma_true'] ==1, 'pred'] = 'gamma'\n",
    "gamma_data.loc[gamma_data['neutron_true'] ==1, 'pred'] = 'neutron'\n",
    "\n",
    "neutron_data=pd.DataFrame([],columns=prob_col)\n",
    "neutron_data['prob']=(proba[:,3])\n",
    "neutron_data['electron_true']=(labels_test[:,0])\n",
    "neutron_data['alpha_true']=(labels_test[:,1])\n",
    "neutron_data['gamma_true']=(labels_test[:,2])\n",
    "neutron_data['neutron_true']=(labels_test[:,3])\n",
    "neutron_data.loc[neutron_data['electron_true'] ==1, 'pred'] = 'electron'\n",
    "neutron_data.loc[neutron_data['alpha_true'] ==1, 'pred'] = 'alpha'\n",
    "neutron_data.loc[neutron_data['gamma_true'] ==1, 'pred'] = 'gamma'\n",
    "neutron_data.loc[neutron_data['neutron_true'] ==1, 'pred'] = 'neutron'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "sns.histplot(ax=ax[0,0],data=electron_data, x=\"prob\",hue='pred', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "ax[0,0].set_xlabel('Probability')\n",
    "ax[0,0].set_title('electron discriminator')\n",
    "\n",
    "\n",
    "sns.histplot(ax=ax[0,1],data=alpha_data, x=\"prob\",hue='pred', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "ax[0,1].set_xlabel('Probability')\n",
    "ax[0,1].set_title('alpha discriminator')\n",
    "\n",
    "\n",
    "sns.histplot(ax=ax[1,0],data=gamma_data, x=\"prob\",hue='pred', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "ax[1,0].set_xlabel('Probability')\n",
    "ax[1,0].set_title('gamma discriminator')\n",
    "\n",
    "\n",
    "sns.histplot(ax=ax[1,1],data=neutron_data, x=\"prob\",hue='pred', element=\"poly\", common_norm=False, stat=\"density\")\n",
    "ax[1,1].set_xlabel('Probability')\n",
    "ax[1,1].set_title('neutron discriminator')\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6b113e-4e6c-44b1-bc98-6535f84eb7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing all metrics\n",
    "columns={'accuracy' : [], 'recall' : [], 'precision' : [], 'F1 score' : [], 'specificity' : []}\n",
    "NN_metrics=pd.DataFrame(np.asarray(NN_metrics_list),columns=columns)\n",
    "NN_metrics=NN_metrics.round(decimals = 5)\n",
    "NN_metrics['method']='multiclassifier'\n",
    "NN_metrics['sample']=['electron','alpha','RockGamma','neutron']\n",
    "cut_metrics['method']='cut'\n",
    "comparison_metrics=pd.concat([NN_metrics,pde_metrics,cut_metrics]).reset_index()\n",
    "comparison_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecb80b-ac59-4bd4-ade8-7e230de2f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparing \n",
    "\n",
    "plt.close()\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12, 8))\n",
    "ax[0,0].plot([0, 1], [0, 1],linestyle='dashdot',color='black', label='No classifier')\n",
    "ax[0,0].plot(fpr[0], tpr[0], linestyle='--',color='orange', label='NN Electrons (area = {:.3f})'.format(auc(fpr[0],tpr[0])))\n",
    "ax[0,0].plot(Bfpr,Btpr,label='PDE beta (area = {:.3f})' .format(auc(Bfpr,Btpr)),color = 'orange')\n",
    "ax[0,0].set_xlabel('False positive rate')\n",
    "ax[0,0].set_ylabel('True positive rate')\n",
    "ax[0,0].set_title('Electron ROC curve')\n",
    "ax[0,0].legend(loc='best')\n",
    "\n",
    "\n",
    "ax[0,1].plot([0, 1], [0, 1],linestyle='dashdot',color='black', label='No classifier')\n",
    "ax[0,1].plot(fpr[1], tpr[1], linestyle='--',color='green', label='NN Alphas (area = {:.3f})'.format(auc(fpr[1],tpr[1])))\n",
    "ax[0,1].plot(Afpr,Atpr,label='PDE alpha (area = {:.3f})' .format(auc(Afpr,Atpr)), color = 'green')\n",
    "ax[0,1].set_xlabel('False positive rate')\n",
    "ax[0,1].set_ylabel('True positive rate')\n",
    "ax[0,1].set_title('Alpha ROC curve')\n",
    "ax[0,1].legend(loc='best')\n",
    "\n",
    "ax[1,0].plot([0, 1], [0, 1],linestyle='dashdot',color='black', label='No classifier')\n",
    "ax[1,0].plot(fpr[2], tpr[2], linestyle='--',color='blue', label='NN Gamma (area = {:.3f})'.format(auc(fpr[2],tpr[2])))\n",
    "ax[1,0].plot(Rfpr,Rtpr,label='PDE rockgamma (area = {:.3f})  ' .format(auc(Rfpr,Rtpr)),color = 'blue')\n",
    "ax[1,0].set_xlabel('False positive rate')\n",
    "ax[1,0].set_ylabel('True positive rate')\n",
    "ax[1,0].set_title('Rockgamma ROC curve')\n",
    "ax[1,0].legend(loc='best')\n",
    "\n",
    "ax[1,1].plot([0, 1], [0, 1],linestyle='dashdot',color='black', label='No classifier')\n",
    "ax[1,1].plot(fpr[3], tpr[3], linestyle='--',color='red', label='NN Neutrons (area = {:.3f})'.format(auc(fpr[3],tpr[3])))\n",
    "ax[1,1].plot(Nfpr,Ntpr,label='PDE neutron (area = {:.3f}) ' .format(auc(Nfpr,Ntpr)),color = 'red')\n",
    "ax[1,1].set_xlabel('False positive rate')\n",
    "ax[1,1].set_ylabel('True positive rate')\n",
    "ax[1,1].set_title('Neutron ROC curve')\n",
    "ax[1,1].legend(loc='best')\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.2, \n",
    "                    hspace=0.3)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
